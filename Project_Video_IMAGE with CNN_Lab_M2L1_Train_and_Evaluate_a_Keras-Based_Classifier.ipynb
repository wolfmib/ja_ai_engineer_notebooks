{"cells":[{"cell_type":"markdown","id":"3dff419f-5632-47ba-b342-de1e7a9536e1","metadata":{},"outputs":[],"source":["<div style=\"text-align: center;\">\n","  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n","  </a>\n","</div>\n"]},{"cell_type":"markdown","id":"80e3d9bb-7ca0-4497-883f-8ceee0420703","metadata":{},"outputs":[],"source":["<h1 align=left><font size = 6>Lab: Train and Evaluate a Keras-Based Classifier </font></h1>\n"]},{"cell_type":"markdown","id":"39708660-b3bd-421a-ba68-5c300fe445fb","metadata":{},"outputs":[],"source":["<h5>Estimated time: 90 minutes</h5>\n"]},{"cell_type":"markdown","id":"6afc14f2-0c74-49cf-ade7-020ee372ea87","metadata":{},"outputs":[],"source":["<h2>Objective</h2>\n","\n","\n","After completing this lab, you will be able to:\n","<ul> \n","    \n","1. Create a Keras-based convolutional neural network (CNN) model.\n","2. Train the CNN model on agricultural and non-agricultural land dataset.\n","3. Evaluate the performance of the CNN model. \n","    \n","</ul> \n"]},{"cell_type":"markdown","id":"0a47bc76-d95e-4eef-8f85-ff1c1ea1fbbb","metadata":{},"outputs":[],"source":["## Introduction\n","\n","This notebook demonstrates the process of building, training, and evaluating a **Keras-based convolutional neural network (CNN)** for image classification, for agricultural images in our case. This lab will cover the following:\n","1. Data preparation\n","2. Model architecture definition\n","3. Training\n","4. Model performance analysis.\n","\n","The goal is to classify satellite images into two categories: \"agricultural\" and \"non-agricultural\"\n"]},{"cell_type":"markdown","id":"529b3315-53ef-4edd-9906-c48cf6d8b285","metadata":{},"outputs":[],"source":["## Table of Contents\n","<font size = 3> \n","\n","1. [Configuration and library imports](#Configuration-and-library-imports)\n","2. [Data acquisition and preparation](#Data-acquisition-and-preparation)\n","3. [Model definition and compilation](#Model-definition-and-compilation)\n","4. [Model training](#Model-training)\n","5. [Download and save the model](#Download-and-save-the-trained-model)\n","6. [Model evaluation and visualization](#Model-evaluation-and-visualization)\n","\n","</font>\n"]},{"cell_type":"markdown","id":"c0d980e7","metadata":{},"outputs":[],"source":["## Configuration and library imports\n"]},{"cell_type":"markdown","id":"654b2c08-19fa-4100-a133-3b9ff87a834e","metadata":{},"outputs":[],"source":["### Install required libraries\n","\n","Some of the required libraries are __not__ pre-installed in the Skills Network Labs environment. You must run the following cell to install them; it might take a few minutes for the installation.\n"]},{"cell_type":"code","id":"ebb28acb-eb30-43ec-ad4d-ed929accf500","metadata":{},"outputs":[],"source":["# define a function to check for successful installation of the libraries\ndef lib_installation_check(captured_data, n_lines_print):\n    \"\"\"\n    A function to use the %%capture output from the cells where we try to install the libraries.\n    It would print last \"n_lines_print\" if there is an error in library installation\n    \"\"\"\n    output_text = captured_data.stdout\n    lines = output_text.splitlines()\n    output_last_n_lines = '\\n'.join(lines[-n_lines_print:])\n    if \"error\" in output_last_n_lines.lower():\n        print(\"Library installation failed!\")\n        print(\"--- Error Details ---\")\n        print(output_last_n_lines)\n    else:\n        print(\"Library installation was successful, let's proceed ahead\")    "]},{"cell_type":"markdown","id":"784459c1-e4cc-4364-a7fc-a0158cf5a9ff","metadata":{},"outputs":[],"source":["### Library installation - 1\n"]},{"cell_type":"markdown","id":"4a973522-25e8-4411-8fc0-f556e5368de7","metadata":{},"outputs":[],"source":["Next, letâ€™s install the non-AI libraries.\n"]},{"cell_type":"code","id":"a2670034-d505-4c6e-8447-4f2ce3f1c722","metadata":{},"outputs":[],"source":["%%time\n%%capture captured_output\n%pip install numpy==1.26\n%pip install matplotlib==3.9.2\n%pip install skillsnetwork"]},{"cell_type":"markdown","id":"0b98e265-dd43-42c9-8cd7-ba7e9458ba5e","metadata":{},"outputs":[],"source":["Now, check if the above libraries are installed properly.\n"]},{"cell_type":"code","id":"3d3b63ba-cdda-411c-8dc0-0f8d2cd528c5","metadata":{},"outputs":[],"source":["lib_installation_check(captured_data = captured_output, n_lines_print = 5)"]},{"cell_type":"markdown","id":"8b935986-bf16-4b71-96f6-2761f2f3ff08","metadata":{},"outputs":[],"source":["### `TensorFlow` library installation\n"]},{"cell_type":"markdown","id":"4743fb43-7101-460c-afc7-b3bacbaf2354","metadata":{},"outputs":[],"source":["Next, install the `TensorFlow` library using the code below. \n"]},{"cell_type":"code","id":"a6beced9-f72c-4dc0-9c5e-a053095c4d1d","metadata":{},"outputs":[],"source":["%%time\n%pip install tensorflow==2.19"]},{"cell_type":"markdown","id":"f7daaf1f-8e40-4bb4-bf84-55e0231e7061","metadata":{},"outputs":[],"source":["### `scikit-learn` library installation\n"]},{"cell_type":"markdown","id":"88a370c3-e436-40f9-846f-7c3a0aabfedd","metadata":{},"outputs":[],"source":["Install the scikit-learn library. \n"]},{"cell_type":"code","id":"7a71610f-410e-402c-8fc4-83e487d71ba8","metadata":{},"outputs":[],"source":["%pip install scikit-learn==1.7.0"]},{"cell_type":"markdown","id":"5abde24a-e929-4a2f-80d2-4dfc2735d140","metadata":{},"outputs":[],"source":["### Import libraries\n"]},{"cell_type":"markdown","id":"dad78903-2d49-410c-900c-eacac5bfeff1","metadata":{},"outputs":[],"source":["Import the non-AI libraries. \n"]},{"cell_type":"code","id":"a75d5a6e","metadata":{},"outputs":[],"source":["import warnings\nwarnings.filterwarnings('ignore')"]},{"cell_type":"code","id":"d385b445","metadata":{},"outputs":[],"source":["import os\nimport sys\nimport time\nimport shutil\nimport random\nimport numpy as np\nimport skillsnetwork\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"5c356149","metadata":{},"outputs":[],"source":["### TensorFlow environment configuration\n","\n","This cell sets environment variables for TensorFlow. \n","- `TF_ENABLE_ONEDNN_OPTS` is set to \"0\" to disable Intel oneDNN optimizations, which can sometimes lead to issues or unwanted behavior on specific hardware configurations.\n","- `TF_CPP_MIN_LOG_LEVEL` is set to \"2,\" instructing TensorFlow to only display warning and error messages from its C++ backend. This reduces verbose output and keeps the console cleaner, focusing on more critical information during model training.\n"]},{"cell_type":"code","id":"119fe8e3","metadata":{},"outputs":[],"source":["os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"]},{"cell_type":"markdown","id":"f4bfe70f","metadata":{},"outputs":[],"source":["Next, set up the data extraction directory.\n"]},{"cell_type":"code","id":"1e40eeac","metadata":{},"outputs":[],"source":["extract_dir = \".\""]},{"cell_type":"markdown","id":"6ace2d9c","metadata":{},"outputs":[],"source":["## Data acquisition and preparation\n","\n","### Define the dataset URL\n","\n","\n","We define the `url` that holds the link to the dataset. The dataset is a `.tar` archive hosted on a cloud object storage service. Cloud object storage (such as S3) is a highly scalable and durable way to store and retrieve large amounts of unstructured data.\n"]},{"cell_type":"code","id":"a30f5074","metadata":{},"outputs":[],"source":["url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\""]},{"cell_type":"markdown","id":"df4712e2","metadata":{},"outputs":[],"source":["### Download the data\n","\n","1. Download and extract data from the cloud using `skillsnetwork.prepare` method.\n","2. Use a fallback method if the `skillsnetwork.prepare` command fails to download and extract the dataset. The fallback involves asynchronously downloading the `.tar` file using `httpx` and then extracting its contents using the `tarfile` library.\n","3. The `tarfile` module provides an interface to tar archives, supporting various compression formats such as gzip and bzip2 (handled by `r:*` mode).\n"]},{"cell_type":"code","id":"38e2b732-75ad-4fd1-b892-222e8208a224","metadata":{},"outputs":[],"source":["def check_skillnetwork_extraction(extract_dir):\n    \"\"\" function to check whether data download and extraction method \n    `skillsnetwork.prepare` would execute successfully, without downloading any data.\n    This helps in early detection and fast fallback to explicit download and extraction\n    using default libraries\n    ###This is a hack for the code to run on non-cloud computing environment without errors\n    \"\"\"\n    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n    if not os.path.exists(symlink_test):\n        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test) \n        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n    os.unlink(symlink_test)\n\nasync def download_tar_dataset(url, tar_path, extract_dir):\n    \"\"\"function to explicitly download and extract the dataset tar file from cloud using native python libraries\n    \"\"\"\n    if not os.path.exists(tar_path): # download only if file not downloaded already\n        try:\n            print(f\"Downloading from {url}...\")\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url, follow_redirects=True)# Download the file asynchronously\n                response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n            \n                with open(tar_path , \"wb\") as f:\n                    f.write(response.content) # Save the downloaded file\n                print(f\"Successfully downloaded '{file_name}'.\")\n        except httpx.HTTPStatusError as http_err:\n            print(f\"HTTP error occurred during download: {http_err}\")\n        except Exception as download_err:\n            print(f\"An error occurred during the fallback process: {download_err}\")\n    else:\n        print(f\"dataset tar file already downloaded at: {tar_path}\")\n    with tarfile.open(tar_path, 'r:*') as tar_ref:\n        tar_ref.extractall(path=extract_dir)\n    print(f\"Successfully extracted to '{extract_dir}'.\")\n            \n"]},{"cell_type":"code","id":"2eea70fc-ca44-474f-950c-b8ffd86ca779","metadata":{},"outputs":[],"source":["try:\n    check_skillnetwork_extraction(extract_dir)\n    await skillsnetwork.prepare(url = url, path = extract_dir, overwrite = True)\nexcept Exception as e:\n    print(e)\n    # --- FALLBACK METHOD FOR DOWNLOADING THE DATA ---\n    print(\"Primary download/extration method failed.\")\n    print(\"Falling back to manual download and extraction...\")\n    \n    # import libraries required for downloading and extraction\n    import tarfile\n    import httpx \n    from pathlib import Path\n    \n    file_name = Path(url).name# Get the filename from the URL (for example, 'data.tar')\n    tar_path = os.path.join(extract_dir, file_name)\n    print(f\"tar_path: {os.path.exists(tar_path)} ___ {tar_path}\")\n    await download_tar_dataset(url, tar_path, extract_dir)"]},{"cell_type":"markdown","id":"1bfbf20f","metadata":{},"outputs":[],"source":["### Import deep learning and ML libraries\n","\n","Here is a brief description of the usage of the **Keras** libraries and methods that will be used:\n","- `Sequential` models are a linear stack of layers.\n","- `Conv2D` and `MaxPooling2D` are fundamental for CNNs, extracting features and reducing dimensionality.\n","- `BatchNormalization` stabilizes training.\n","- `Dense` layers form the classifier.\n","- `Dropout` regularizes to prevent overfitting.\n","- `Adam` is an adaptive learning rate optimizer.\n","- `ImageDataGenerator` automates data loading and augmentation.\n","- `HeUniform` is used for weight initialization.\n","\n","\n","**Scikit-learn** (`sklearn.metrics`) provides the following metrics for model performance assessment: \n","- `classification_report`\n","- `confusion_matrix`\n","- `accuracy_score`\n"]},{"cell_type":"code","id":"3d220f98","metadata":{},"outputs":[],"source":["import tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.initializers import HeUniform\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Succesfully imported the libraries\")"]},{"cell_type":"markdown","id":"003882d4-4301-4dde-a0b6-a071533b0766","metadata":{},"outputs":[],"source":["### Get the processing device\n","Check the availability of GPU\n"]},{"cell_type":"code","id":"c2908110-98d2-4890-b7f7-5a11f82ad859","metadata":{},"outputs":[],"source":["gpu_list = tf.config.list_physical_devices('GPU')\n\ndevice = \"gpu\" if gpu_list !=[] else \"cpu\"\nprint(f\"Device available for training: {device}\")"]},{"cell_type":"markdown","id":"de9da38a","metadata":{},"outputs":[],"source":["### Reproducibility with random seeds\n","\n","Here we fix the random seeds for `random` module, NumPy, and TensorFlow. By initializing these seeds with a constant value (for example, 42), any operations that involve randomness (such as weight initialization, data shuffling, or data augmentation) will produce the exact same sequence of random numbers every time the code is run. This is crucial for ensuring the reproducibility of experimental results and when comparing different models or hyperparameters.\n"]},{"cell_type":"code","id":"82346e86","metadata":{},"outputs":[],"source":["# Set seed for reproducibility\nseed_value = 7331\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)"]},{"cell_type":"markdown","id":"abc13d4d","metadata":{},"outputs":[],"source":["### Define the dataset path\n"]},{"cell_type":"code","id":"a5f6c8eb","metadata":{},"outputs":[],"source":["dataset_path = os.path.join(extract_dir, \"images_dataSAT\")\nprint(dataset_path)"]},{"cell_type":"markdown","id":"fcda9646","metadata":{},"outputs":[],"source":["### Create the dataset file list\n"]},{"cell_type":"markdown","id":"4084fba0-b7b1-4a5c-baa6-c88a0ff34f6b","metadata":{},"outputs":[],"source":["Now that we have downloaded the dataset, perform the following task. \n"]},{"cell_type":"markdown","id":"61942ff0-5514-4b3d-8173-861885df05b3","metadata":{},"outputs":[],"source":["### **Task 1:** Recursively walk through the `dataset_path` using `os.walk` function to create a list **`fnames`** of all image files. \n","Print the total count of files found and displays the first two and last two file paths. \n","\n","Absolute path is captured using `os.path.join(dirname, filename)` and used in `ImageDataGenerator` later.\n"]},{"cell_type":"code","id":"6a2f2b69-ee38-4174-b86f-694d6377a84d","metadata":{},"outputs":[],"source":["## You can use this cell to type the code to complete the task.\n"]},{"cell_type":"markdown","id":"9398d118-7eac-41e4-aaa1-3771afcb82fe","metadata":{},"outputs":[],"source":["Double-click **here** for the solution.\n","<!-- The correct answer is:\n","\n","\n","fnames = []\n","for dirname, _, filenames in os.walk(dataset_path):\n","    for filename in filenames:\n","        fnames.append(os.path.join(dirname, filename))\n","print(f\"total files in dataset: {len(fnames)}\")\n","nfname_print=2\n","for f in fnames[:nfname_print]:\n","    print(f)\n","for f in fnames[-nfname_print:]:\n","    print(f)\n","\n","-->\n"]},{"cell_type":"markdown","id":"eb34fa2a","metadata":{},"outputs":[],"source":["### Define the model hyperparameters\n","\n","Hyperparameters are configurable values that are set before the training process begins. \n","\n","This cell initializes several key hyperparameters that will govern the training process and the model's input. Here is the list of hyperparameters:\n","\n","1. `img_w` and `img_h` define the width and height for resizing input images.\n","2. `n_channels` defines the number of color channels (3 for RGB).\n","3. `n_epochs` sets the total training iterations over the dataset.\n","4. `batch_size` sets the number of samples processed per batch in the epoch.\n","5. `lr` defines the learning rate for the optimizer.\n","6. `steps_per_epoch` are total number of steps used for training. **None** means the number is calculated automatically.\n","7. `validation_steps` are total number of steps used for validating the model on validation data. **None** means the number is calculated automatically.\n","\n","These hyperparameters are crucial for controlling model performance and resource utilization and significantly influence a model's performance and training efficiency. \n"]},{"cell_type":"code","id":"97eae79e","metadata":{},"outputs":[],"source":["img_w, img_h = 64, 64\nn_channels = 3\nbatch_size = 128\nlr = 0.001 # Learning rate\nn_epochs = 3 # set to low number for your convenience. You can change this to any number of your liking\n\nsteps_per_epoch = None\nvalidation_steps = None \n\nmodel_name = \"ai_capstone_keras_best_model.model.keras\""]},{"cell_type":"markdown","id":"1aba0671","metadata":{},"outputs":[],"source":["### Configure `ImageDataGenerator` for Augmentation\n","\n","\n","Now, we instantiate the `ImageDataGenerator` with data augmentation parameters:\n","\n","- `rescale=1./255` normalizes pixel values to [0, 1].\n","- `rotation_range`, `width_shift_range`, `height_shift_range`, `shear_range`, and `zoom_range` define random transformations to apply to images during training, increasing dataset diversity.\n","- `horizontal_flip=True` enables random horizontal mirroring.\n","- `fill_mode='nearest'` specifies how new pixels are filled after transformations.\n","- `validation_split=0.2` reserves 20% of data for validation.\n","\n","\n","This setup boosts model robustness against variations in real-world images. `ImageDataGenerator` performs these transformations on-the-fly, making it efficient for large datasets. \n"]},{"cell_type":"code","id":"88cc5bc9","metadata":{},"outputs":[],"source":["datagen = ImageDataGenerator(rescale=1./255,\n                             rotation_range=40, \n                             width_shift_range=0.2,\n                             height_shift_range=0.2,\n                             shear_range=0.2,\n                             zoom_range=0.2,\n                             horizontal_flip=True,\n                             fill_mode=\"nearest\",\n                             validation_split=0.2\n                            )"]},{"cell_type":"markdown","id":"44d2ce28","metadata":{},"outputs":[],"source":["### Create training and validation data generators\n","\n"," `ImageDataGenerator` is used to create `train_generator` and `validation_generator`. \n","`flow_from_directory()` is a convenient method of `ImageDataGenerator` for automatically creating data pipelines from structured image directories.\n"," The generator resize images to `(img_w, img_h)` and group them into `batch_size` chunks. `class_mode=\"binary\"` indicates a two-class classification task. \n"," \n"," The `subset` parameter is used to assign 80% of the data for training and 20% for validation based on the `validation_split`. \n"]},{"cell_type":"code","id":"57ff1974-13dc-4fce-842b-041edbb2cfc8","metadata":{},"outputs":[],"source":["train_generator = datagen.flow_from_directory(dataset_path,\n target_size = (img_w, img_h),\n batch_size= batch_size,\n class_mode=\"binary\",\n subset=\"training\"\n )"]},{"cell_type":"markdown","id":"c25805f5-962c-42dc-9d90-cb95065a101e","metadata":{},"outputs":[],"source":["Here is your next task. We have created the `train_generator`, let's create the `validation_generator`.\n"]},{"cell_type":"markdown","id":"e0e7b840-63d3-4881-9a65-8f0b9f8d9894","metadata":{},"outputs":[],"source":["### **Task 2:** Create the `validation_generator` from `dataset_path`.\n","Use `target_size`, and `class_mode` similar to `train_generator`.\n"]},{"cell_type":"code","id":"3801e435-d4bf-4630-ab47-24f8c75c6d2c","metadata":{},"outputs":[],"source":["## You can use this cell to type the code to complete the task.\n"]},{"cell_type":"markdown","id":"efa6cafb-8064-40a1-83c2-c1a370c00976","metadata":{},"outputs":[],"source":["Double-click **here** for the solution.\n","<!-- The correct answer is:\n","validation_generator = datagen.flow_from_directory(dataset_path,\n","                                                    target_size =(img_w, img_h),\n","                                                    batch_size = batch_size, \n","                                                    class_mode=\"binary\",\n","                                                    subset=\"validation\"\n","                                                    )\n","\n","-->\n"]},{"cell_type":"markdown","id":"df897734","metadata":{},"outputs":[],"source":["## Model definition and compilation\n","\n","### Define the convolutional neural network (CNN) architecture\n","\n","The model architecture is composed of several key components:\n","- **`Sequential`** is a linear stack of layers in Keras.\n","- **Conv2D** layers perform convolution operations, acting as feature detectors.\n","- **MaxPooling2D** reduces the spatial dimensions of the feature maps.\n","-  **BatchNormalization** normalizes layer inputs, stabilizing and accelerating training.\n","-  **GlobalAveragePooling2D** summarizes feature maps into a single vector, reducing parameters.\n","-  **Dense** (fully connected) layers learn complex patterns from these features.\n","-  **Dropout** is a regularization technique that randomly deactivates neurons during training.\n","-  **Sigmoid** activation is used for binary classification, mapping outputs to probabilities.\n","-  **HeUniform** initializer is suitable for ReLU activations.\n","-  **The final output `Dense` layer** uses a `sigmoid` activation for binary classification, outputting a probability between 0 and 1.\n"]},{"cell_type":"code","id":"7197185c","metadata":{},"outputs":[],"source":["model = Sequential([\n                    Conv2D(32 , (5,5) , activation=\"relu\",padding=\"same\",strides=(1,1), kernel_initializer=HeUniform(), input_shape=(img_w, img_h, n_channels)),\n                    MaxPooling2D(2,2),\n                    BatchNormalization(),\n                    \n                    Conv2D(64, (5,5) , activation=\"relu\",padding=\"same\" , strides=(1,1), kernel_initializer=HeUniform()),\n                    MaxPooling2D(2,2),\n                    BatchNormalization(),\n                    \n                    Conv2D(128, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n                    MaxPooling2D(2,2),\n                    BatchNormalization(),\n                    \n                    ###\n                    Conv2D(256, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n                    MaxPooling2D(2,2),\n                    BatchNormalization(),\n                    \n                    Conv2D(512, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n                    MaxPooling2D(2,2),\n                    BatchNormalization(),\n                    \n                    Conv2D(1024, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n                    MaxPooling2D(2,2),\n                    BatchNormalization(),\n                    \n                    \n                    ###\n                    GlobalAveragePooling2D(),\n                    \n                    Dense(64,activation=\"relu\" , kernel_initializer=HeUniform()),\n                    BatchNormalization(),\n                    Dropout(0.4),\n                    \n                    Dense(128,activation=\"relu\" , kernel_initializer=HeUniform()),\n                    BatchNormalization(),\n                    Dropout(0.4),\n                    \n                    Dense(256,activation=\"relu\" , kernel_initializer=HeUniform()),\n                    BatchNormalization(),\n                    Dropout(0.4),\n                    \n                    ###\n                    Dense(512,activation=\"relu\" , kernel_initializer=HeUniform()),\n                    BatchNormalization(),\n                    Dropout(0.4),\n                    \n                    Dense(1024,activation=\"relu\" , kernel_initializer=HeUniform()),\n                    BatchNormalization(),\n                    Dropout(0.4),\n                    \n                    Dense(2048,activation=\"relu\" , kernel_initializer=HeUniform()),\n                    BatchNormalization(),\n                    Dropout(0.4),\n                    \n                    \n                    ###\n                    Dense(1 , activation=\"sigmoid\")\n                    \n                ])"]},{"cell_type":"markdown","id":"2c59dda6","metadata":{},"outputs":[],"source":["### Compile the model and display the summary\n","\n","\n","Here, we compile the model using `model.compile()` with the `Adam` optimizer and `learning_rate` equal to `lr` (0.001). \n","\n","The `loss` function is specified as `\"binary_crossentropy\"`, appropriate for binary classification problems. \n","`accuracy` is set as the performance `metric` to monitor training and evaluation. \n","We print `model.summary()` for a detailed overview of the network\n"]},{"cell_type":"code","id":"cc7a234a","metadata":{},"outputs":[],"source":["loss = \"binary_crossentropy\"\nmodel.compile(optimizer=Adam(learning_rate=lr),\n              loss=loss, \n              metrics=[\"accuracy\"])\n\nprint(model.summary())"]},{"cell_type":"markdown","id":"602ace3c-0159-44f9-a386-6e2586f0d90c","metadata":{},"outputs":[],"source":["Answer the question below in the space provided. \n"]},{"cell_type":"markdown","id":"f97e78c1-121b-4461-a440-2505bf8faa48","metadata":{},"outputs":[],"source":["## Question: Count the total number of layers in this CNN model?\n"]},{"cell_type":"markdown","id":"d5420f7c-8738-476c-bea7-db19a38a395e","metadata":{},"outputs":[],"source":["### You can use this cell to type the answer to the question.\n"]},{"cell_type":"markdown","id":"a3be625e-e99d-454f-a945-4d788df6578f","metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","id":"fbffce73-7f2d-4cd2-8ca3-58f5e6480587","metadata":{},"outputs":[],"source":["Double-click **here** for the solution.\n","<!-- The correct answer is:\n","There are total 38 layers.\n","-->\n"]},{"cell_type":"markdown","id":"cce0cf5c-d6b5-4889-be90-0aa6c61be7fc","metadata":{},"outputs":[],"source":["You now know how to create a CNN model using Keras. You can perform the following task. \n"]},{"cell_type":"markdown","id":"bc991a22-1b16-4dc8-add3-cb36d194942c","metadata":{},"outputs":[],"source":["## Task 3: Create and compile a CNN model `test_model` with 4 Conv2D layers and 5 Dense layers.\n"]},{"cell_type":"code","id":"a84fd320-0dd2-4b37-a223-2f3bb37583ce","metadata":{},"outputs":[],"source":["## You can use this cell to type the code to complete the task.\n"]},{"cell_type":"markdown","id":"f3dd6c52-9b24-4184-a133-f6c78203260b","metadata":{},"outputs":[],"source":["Double-click **here** for the solution.\n","<!-- The correct answer is:\n","model = Sequential([\n"," Conv2D(32 , (5,5) , activation=\"relu\",padding=\"same\",strides=(1,1), kernel_initializer=HeUniform(), input_shape=(img_w, img_h, n_channels)),\n"," MaxPooling2D(2,2),\n"," BatchNormalization(),\n","\n"," Conv2D(64, (5,5) , activation=\"relu\",padding=\"same\" , strides=(1,1), kernel_initializer=HeUniform()),\n"," MaxPooling2D(2,2),\n"," BatchNormalization(),\n","\n"," Conv2D(128, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n"," MaxPooling2D(2,2),\n"," BatchNormalization(),\n"," \n","###\n"," Conv2D(256, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n"," MaxPooling2D(2,2),\n"," BatchNormalization(),\n"," \n","###\n"," GlobalAveragePooling2D(),\n","\n"," Dense(64,activation=\"relu\" , kernel_initializer=HeUniform()),\n"," BatchNormalization(),\n"," Dropout(0.4),\n","\n"," Dense(128,activation=\"relu\" , kernel_initializer=HeUniform()),\n"," BatchNormalization(),\n"," Dropout(0.4),\n","\n"," Dense(256,activation=\"relu\" , kernel_initializer=HeUniform()),\n"," BatchNormalization(),\n"," Dropout(0.4),\n","\n","###\n"," Dense(512,activation=\"relu\" , kernel_initializer=HeUniform()),\n"," BatchNormalization(),\n"," Dropout(0.4),\n","\n","###\n"," Dense(1 , activation=\"sigmoid\")\n"," \n"," ])\n","\n","# Compile the model to make it ready for training\n","loss = \"binary_crossentropy\"\n","model.compile(optimizer=Adam(learning_rate=lr),loss=loss, metrics=[\"accuracy\"])\n","\n","-->\n"]},{"cell_type":"markdown","id":"b7045b4a","metadata":{},"outputs":[],"source":["## Model training\n","\n","### Display the training configuration and hyperparameters\n","\n","Here we print a comprehensive summary of the training configuration and list all critical hyperparameters. This detailed output serves as a quick reference and verification of the experimental setup.\n","Before commencing computationally intensive tasks such as deep learning model training, it's a good practice to log and verify the configuration.\n"]},{"cell_type":"code","id":"a9f7cb52","metadata":{},"outputs":[],"source":["print(f\"Training Hyperparameters:\\n\\\n        n_classes (train) = {train_generator.num_classes},\\n\\\n        n_classes (validation) = {validation_generator.num_classes},\\n\\\n        img_w, img_h ={img_w, img_h},\\n\\\n        n_channels = {n_channels},\\n\\\n        batch_size = {batch_size},\\n\\\n        steps_per_epoch = {steps_per_epoch},\\n\\\n        n_epochs = {n_epochs},\\n\\\n        validation_steps = {validation_steps},\\n\\\n        learning_rate = {lr}\")"]},{"cell_type":"markdown","id":"2043d82b-6bfb-4fab-bfc2-69c0d943c491","metadata":{},"outputs":[],"source":["### Save the model checkpoint\n","\n","Now we declare a method to save the **best model** during training. The best model can be defined by either **lowest loss** or **high accuracy**.\n"]},{"cell_type":"code","id":"5d7030d8-a5fa-4c3d-8da7-e32bf78b0f9c","metadata":{},"outputs":[],"source":["# Create the ModelCheckpoint callback\ncheckpoint_cb = ModelCheckpoint(filepath=model_name,\n                                monitor='val_loss',      # or 'val_accuracy'\n                                mode='min',              # 'min' for loss, 'max' for accuracy\n                                save_best_only=True,\n                                verbose=1\n                               )"]},{"cell_type":"markdown","id":"4dd6d845-24ce-4492-9b7a-e7b96798bb08","metadata":{},"outputs":[],"source":["The checkpoint of a model can also be based on high accuracy. So, here's your next task.  \n"]},{"cell_type":"markdown","id":"73ed3e8b-5ec0-41df-a76c-4239fff66f94","metadata":{},"outputs":[],"source":["### **Task 4**: Create the checkpoint callback for model with **maximum accuracy**. \n"]},{"cell_type":"code","id":"e356f45b-0c7d-4403-83f9-16a797cd72fe","metadata":{},"outputs":[],"source":["## You can use this cell to type the code to complete the task.\n"]},{"cell_type":"markdown","id":"6156341a-60a0-4cd6-84ae-3bbe81262cda","metadata":{},"outputs":[],"source":["Double-click **here** for the solution.\n","<!-- The correct answer is:\n","checkpoint_cb = ModelCheckpoint(filepath=model_name,\n","                                monitor='val_accuracy',\n","                                mode='max',\n","                                save_best_only=True,\n","                                verbose=1\n","                               )\n","\n","\n","-->\n"]},{"cell_type":"markdown","id":"6220ec0c","metadata":{},"outputs":[],"source":["### Execute model training\n","\n","- `model.fit()` is the primary function for training a Keras model. It controls the entire training loop: iterating over epochs, fetching data batches from generators, performing forward and backward passes, updating weights via the optimizer, and calculating loss and metrics.\n","- `steps_per_epoch` (*if specified*) determines how many batches constitute an \"epoch.\"\n","- `validation_data` and `validation_steps` allow monitoring of the model's generalization ability on a separate dataset, helps in detecting overfitting.\n","- `callbacks` determines how the best model is saved.\n","- The `fit` object stores the model's training history.\n"]},{"cell_type":"code","id":"a9e84af4","metadata":{},"outputs":[],"source":["print(f\"Training on : ==={device}=== with batch size: {batch_size} & lr: {lr}\")\n\nfit = model.fit(train_generator, \n                epochs= n_epochs,\n                steps_per_epoch = steps_per_epoch,\n                validation_data=(validation_generator),\n                validation_steps = validation_steps,\n                callbacks=[checkpoint_cb],\n                verbose=1\n               )"]},{"cell_type":"markdown","id":"11207c3c-3d98-4294-8f59-4e4eb1456b26","metadata":{},"outputs":[],"source":["## Download and save the trained model\n"]},{"cell_type":"markdown","id":"f4a5d799-0dcc-4fd1-a586-18196feeb756","metadata":{},"outputs":[],"source":["After the training is completed, you will see `ai_capstone_keras_best_model.model.keras` in the left pane\n","\n","**However**, for your convenience, I have saved a model trained over 20 epochs **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/f63OXPboUBgVhDpozcJZ3w/ai-capstone-keras-best-model-model.keras)**. You can download that for evaluation and further labs on your local machine from **[this link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/f63OXPboUBgVhDpozcJZ3w/ai-capstone-keras-best-model-model.keras)**.\n","\n","\n","This is the Keras AI model created by training on the provided dataset for agricultural and non-agricultural land dartaset. This model can now be used for infering un-classified images with the dimensions similar to the training images. \n","\n","- You can also download the your trained model file: `ai_capstone_keras_best_model.model.keras` from the left pane and save it on your local computer. \n","- You can download this model by \"right-click\" on the file and then Clickinng \"Download\".\n","- You could use this model for the other labs of this capstone project course.\n","\n","\n","Please refer to the screenshots below for downloading the model to your local computer.\n","\n","\n","### The trained model file (`ai_capstone_keras_best_model.model.keras` ) in the left pane\n","![Model_Keras_download_screenshot_1_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/NM4wJ1o8G3f0Gv3Ic_cHOQ/Model-Keras-download-screenshot-1-marked.png)\n","\n","\n","### The **download** option\n","![Model_Keras_download_screenshot_2_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/y4ubxvX6OHSWP9KvB-fzHQ/Model-Keras-download-screenshot-2-marked.png)\n"]},{"cell_type":"markdown","id":"c095e5f6-4687-4cb4-9263-4a081f133021","metadata":{},"outputs":[],"source":["## Model evaluation and visualization\n"]},{"cell_type":"markdown","id":"b7988c04","metadata":{},"outputs":[],"source":["### Perform a comprehensive model evaluation\n","\n","Here, you will perform a detailed evaluation of the trained model on the validation dataset. You would calculate the necessary prediction `steps` based on the validation data and `batch_size`. Then, you will obtain the true class labels (`y_true`) and generate the model's predictions (`y_pred`) on the validation set. The predicted probabilities are converted to binary class labels using a 0.5 threshold. Finally, you will print the overall `accuracy_score`, to get a  quantitative assessment of the model's performance on unseen data.\n","\n","Model evaluation metrics are essential for understanding a model's generalization ability. `y_true` represents the actual labels, while `y_pred` are the model's predicted labels. For binary classification, probabilities are converted to class labels by thresholding. The **accuracy score** is the proportion of correct predictions out of the total predictions.\n"]},{"cell_type":"code","id":"40c7fe5d","metadata":{},"outputs":[],"source":["steps = int(np.ceil(validation_generator.samples / validation_generator.batch_size))\nbatch_size = int(validation_generator.batch_size)\n\nall_preds = []\nall_labels = []\nfor step in range(steps):\n    # Get one batch data\n    images, labels = next(validation_generator)\n    preds = model.predict(images)\n    preds = (preds > 0.5).astype(int).flatten() \n    all_preds.extend(preds)\n    all_labels.extend(labels)\naccuracy = accuracy_score(all_labels, all_preds)\nprint(f\"Accuracy Score: {accuracy:.4f}\")"]},{"cell_type":"markdown","id":"f58fc11a-0228-47dc-ac46-521cd739e1b5","metadata":{},"outputs":[],"source":["### Visualize the training history (accuracy and loss)\n","\n","\n","This cell generates two plots to visualize the model's training performance, one for accuracy and one for loss, across epochs. \n","- **Accuracy** measures the proportion of correct predictions. \n","- **Loss** quantifies the error between predictions and true labels. \n","- Using these metrics, we can check the model for **overfitting** or **underfitting**. \n","- `fit.history` attribute stores these metrics for each epoch.\n"]},{"cell_type":"code","id":"b3e85bc7-ce80-4780-a1ee-522aae09bbb9","metadata":{},"outputs":[],"source":["# Create a figure with a subplot\nfig, axs = plt.subplots(figsize=(8, 6))\n\n# Plot Accuracy on the first subplot\naxs.plot(fit.history['accuracy'], label='Training Accuracy')\naxs.plot(fit.history['val_accuracy'], label='Validation Accuracy')\naxs.set_title('Model Accuracy')\naxs.set_xlabel('Epochs')\naxs.set_ylabel('Accuracy')\naxs.legend()\naxs.grid(True)\n\nplt.tight_layout()\nplt.show()"]},{"cell_type":"markdown","id":"bd999097-1580-43b6-99c5-6ba956cabdbd","metadata":{},"outputs":[],"source":["Plot the model loss in the task below. \n"]},{"cell_type":"markdown","id":"084ba209-5b62-4686-8353-ec3c6d3f39bd","metadata":{},"outputs":[],"source":["### **Task 5:** Plot the graph for **training loss** and **validation loss** for the model `fit`.\n"]},{"cell_type":"code","id":"06249605-0d3c-4a36-b53b-9b07924a9e46","metadata":{},"outputs":[],"source":["## You can use this cell to type the code to complete the task.\n"]},{"cell_type":"markdown","id":"3143eeb9-e305-4a39-9598-2bdaf84115c4","metadata":{},"outputs":[],"source":["Double-click **here** for the solution.\n","<!-- The correct answer is:\n","\n","fig, axs = plt.subplots( figsize=(8, 6))\n","\n","\n","# Plot Loss on the second subplot\n","axs.plot(fit.history['loss'], label='Training Loss')\n","axs.plot(fit.history['val_loss'], label='Validation Loss')\n","axs.set_title('Model Loss')\n","axs.set_xlabel('Epochs')\n","axs.set_ylabel('Loss')\n","axs.legend()\n","axs.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n","-->\n"]},{"cell_type":"markdown","id":"45da1a7d-3166-4ded-b9d2-9c46a759e3d6","metadata":{},"outputs":[],"source":["## Save and download the notebook for **final project** submission and evaluation\n","\n","You will need to save and download the completed notebook for final project submission and evaluation. \n","<br>For saving and downloading the completed ntoebook, please follow the steps given below:</br>\n","\n","<font size = 4>  \n","\n","1) **Complete** all the tasks and questions given in the notebook.\n","\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n","\n","2) **Save** the notebook.</style>\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n","\n","3) Identify and right click on the **correct notebook file** in the left pane.</style>\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n","\n","4) Click on **Download**.</style>\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n","\n","5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n","  </font>\n"]},{"cell_type":"markdown","id":"f3ce0319-3189-4db3-aeb7-d8ef19734030","metadata":{},"outputs":[],"source":["## Conclusion\n","Congratulation! You've successfully bulit, trained, and evaluated a deep learning model using Keras for image classification.\n","\n","- **Robust data handling:** We implemented a robust data acquisition strategy, featuring a primary method and a crucial fallback for reliable data downloading and extraction.\n","- **Reproducibility:** We used fixed random seeds ensures your results are consistent and verifiable across multiple runs.\n","- **Data generators:** We learnt about ImageDataGenerator for efficient on-the-fly image loading, resizing, normalization, and vital data augmentation.\n","- **CNN architecture:** We built a multi-layered CNN, incorporating Conv2D, MaxPooling2D, BatchNormalization, Dropout, and Dense layers for effective feature learning and classification.\n","- **Model compilation:** We configured the model's learning process with an Adam optimizer, binary_crossentropy loss, and accuracy metric.\n","- **Training process:** We executed the training loop, feeding data in batches and monitoring performance over epochs.\n","- **Performance visualization:** We plotted the accuracy and loss plots for understandinbg the model's learning progress and identify overfitting.\n","- **Model evaluation:** Finally, we use accuracy_score for a quantitative assessment of your model.\n"]},{"cell_type":"markdown","id":"807370f5-c80c-4498-9955-6e15aad4bdbc","metadata":{},"outputs":[],"source":["<h2>Author</h2>\n","\n","[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n","\n","Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n","\n"]},{"cell_type":"markdown","id":"8e156172-bf48-4869-b01b-4dda220188a0","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","'''|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","```\n","```|---|---|---|---|\n","```\n","```| 2025-06-21  | 1.0  | Aman  |  Created the lab |\n","```\n","```| 2025-06-30  | 2.0  | Sangeeta |  ID review |\n","```\n","-->\n"]},{"cell_type":"markdown","id":"fac1fc1b-0bbd-45e2-9a6a-f06b7b0ced04","metadata":{},"outputs":[],"source":["Â© Copyright IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"prev_pub_hash":"fd4028941667565cdca5d946adb49321584044cacb8975f859bc4f258d5b52fa"},"nbformat":4,"nbformat_minor":5}