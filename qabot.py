## Project How to Create QA Bot with IBM Gradio

from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames
from ibm_watsonx_ai import Credentials
from langchain_ibm import WatsonxLLM, WatsonxEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA

import gradio as gr

# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')


## LLM
def get_llm():
    model_id = 'mistralai/mixtral-8x7b-instruct-v01'
    # define parameters
    parameters = {
        "temperature": 0.5,
        "max_new_tokens": 256
    }
    project_id = "skills-network"
    watsonx_llm = WatsonxLLM(
        model_id=......,
        url="https://us-south.ml.cloud.ibm.com",
        project_id=.......,
        params=.....,
    )
    return watsonx_llm


## Task 1 Document loader
def document_loader(file):
    #Initial pdf loader
    loader = PyPDFLoader(file.name)
    loaded_document = loader.load()
    return loaded_document



## Task2 
## Text splitter
def text_splitter(data):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200, # common choise 20%
        length_function=len,
    )
    chunks = text_splitter.split_documents(data)
    return chunks


## Task3 Embeded documents
## Embedding model
def watsonx_embedding():
    embed_params = {
        "max_new_tokens": 256,
        "temperature": 0.5
    },
    }
    watsonx_embedding = WatsonxEmbeddings(
        model_id="ibm/slate-125m-english-rtrvr",   # IBM embedding model
        url="https://us-south.ml.cloud.ibm.com",  # Watsonx endpoint
        project_id="skills-network",              # Lab project ID
        params=embed_params,
    )
    return watsonx_embedding


## Task4: # Vector DB


def vector_database(chunks):
    embedding_model = watsonx_embedding()
    vectordb = Chroma.from_documents(chunks, embedding_model)
    return vectordb



## Task5:  ## Retriever
def retriever(file):
    splits = document_loader(file)
    chunks = text_splitter(splits)
    vectordb = vector_database(chunks)
    retriever = vectordb.as_retriever()
    return retriever



## Task6  
## QA Chain
def retriever_qa(file, query):
    llm = get_llm()
    retriever_obj = retriever(file)
    qa = RetrievalQA.from_chain_type(llm=llm, 
                                    chain_type='stuff', 
                                    retriever=retriever_obj, 
                                    return_source_documents=True)
    response = qa.invoke({'query': query})
    return response['result']


# setup interface
# Create Gradio interface
# Bridge so our loader (which uses file.name) works with Gradio's filepath
def gradio_rag(pdf_path, query):
    class _F: pass
    f = _F()
    f.name = pdf_path
    return retriever_qa(f, query)


# Create Gradio interface
rag_application = gr.Interface(
    fn=gradio_rag,
    allow_flagging="never",
    inputs=[
        gr.File(label="Upload PDF File", file_count="single", file_types=['.pdf'], type="filepath"),
        gr.Textbox(label="Input Query", lines=2, placeholder="Type your question here...")
    ],
    outputs=gr.Textbox(label="Answer"),
    title="Document QA (RAG) Bot",
    description="Upload a PDF document and ask any question. The chatbot will try to answer using the provided document."
)


"""
Why this works

fn=gradio_rag adapts Gradio’s filepath string into an object with .name so your document_loader keeps working unchanged.

allow_flagging="never" disables flagging per typical lab specs.

Output label set to “Answer” and a clear title for the app.

"""

## launch 
# Launch the app 
rag_application.launch(server_name="0.0.0.0", server_port=7860)


"""
ongratulations! You’ve successfully completed the Generative AI Applications with RAG and LangChain course, where you’ve applied your knowledge to real-world scenarios, refining your skills in using document loaders, text-splitting strategies, vector databases, and retrievers within LangChain. You’ve built a QA bot, integrated a simple Gradio interface, and explored the nuances of retrieval-augmented generation (RAG).

At this point, you know that:

LangChain uses document loaders, which are connectors that gather data and convert it into a compatible format.

TextLoader class is used to load plain text files.

PyPDFLoader class or PyMuPDF Loader is used for PDF files.

UnstructuredMarkdownLoader is used for Markdown files.

Use the JSONLoader class to load JSON files.

CSV Loader is used for CSV files.

Beautiful Soup or WebBaseLoader is used to load and parse an online webpage.

WebBaseLoader is used to load multiple websites.

UnstructuredFileLoader is used for unknown or varied file formats.

LangChain uses text splitters to split a long document into smaller chunks.

Text splitters operate along two axes: The method used to break the text and how the chunk is measured.

Key parameters of a text splitter: Separator, chunk size, chunk overlap, and length function.

Commonly used splitters: Split by Character, Recursively Split by Character, Split Code, and Markdown Header Text Splitter.

Embeddings from data sources can be stored using a vector store.

A vector database retrieves information based on queries using similarity search.

Chroma DB is a vector store supported by LangChain that saves embeddings along with metadata.

To construct the Chroma DB vector database, import the Chroma class from LangChain vector stores and call the chunks and embedding model.

A similarity search process starts with a query, which the embedding model converts into a numerical vector format.

"""

print("See summary for this as well")
