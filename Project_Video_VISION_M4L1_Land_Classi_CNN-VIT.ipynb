{"cells":[{"cell_type":"markdown","id":"5722ff70-0712-470b-bdba-03ce13593849","metadata":{},"outputs":[],"source":["<div style=\"text-align: center;\">\n","  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n","  </a>\n","</div>\n"]},{"cell_type":"markdown","id":"1d05e403-a3fa-472c-b974-132e0759b47a","metadata":{},"outputs":[],"source":["<h1 align=left><font size = 6>Lab: Land Classification: CNN-Transformer Integration evaluation </font></h1>\n","    \n","<h2 align=left><font size = 5>Vision Transformer (ViT) Model Evaluation </font></h2>\n"]},{"cell_type":"markdown","id":"f50a9830-2661-48f4-bff1-fa4aef0fbfed","metadata":{},"outputs":[],"source":["Estimated time: 90 minutes\n"]},{"cell_type":"markdown","id":"3a836b45-82f5-4264-9c53-417b0bddd8b9","metadata":{},"outputs":[],"source":["# Introdution\n","\n","This notebook presents an end-to-end workflow for importing, testing, and evaluating two Vision Transformer (ViT) models developed in Keras and PyTorch, respectively. \n","The self-attention mechanism in the ViTs allows these models to learn complex and broad spatial dependencies, providing improved performance on a variety of vision tasks compared to traditional convolutional neural networks. However, the CNNs are adept in learning the local features very well and can be trained using relatively smaller datasets and is generally much more efficient in utilizing the computational resources, as compared to ViTs. A CNN-ViT hybrid architecture gains from both CNN and ViT model strengths, by getting local features extracted using CNNs, while the transformer part of the hybrid architecture can determine the global dependencies.\n","\n","This lab focuses on model loading, prediction on sample data, and quantitative evaluation of the ViT models created using KEras and PyTorch. You'll explore the details of the framework-specific implementations, test the consistency of results, and gain practical experience comparing deep learning models across different Python ecosystems. \n","\n","Upon completion, you will have a thorough understanding of loading the CNN-ViT hybrid models testing workflow, key evaluation metrics, and how high-level architectural concepts translate into practical model evaluation using both Keras and PyTorch.\n"]},{"cell_type":"markdown","id":"9703a53b-4f03-4f28-939b-65730f879fa3","metadata":{},"outputs":[],"source":["## Objectives\n","\n","- Import and initialize pre-trained CNN - Vision Transformer hybrid models from two deep learning frameworks (Keras/TensorFlow and PyTorch).\n","- Prepare and preprocess sample image data for inference.\n","- Perform model inference and obtain prediction results from both models.\n","- Compute and compare core evaluation metrics.\n"]},{"cell_type":"markdown","id":"4d956a84-eb44-4956-a1e8-eda3209dd38a","metadata":{},"outputs":[],"source":["## Table of Contents\n","1. [Dataset download, extraction and paths](#Dataset-download,-extraction-and-paths)\n","2. [Pre-trained model download](#Pre-trained-model-download)\n","3. [Package installation](#Package-installation)\n","4. [Library imports and setup](#Library-imports-and-setup)\n","5. [Fix random seed for reproducibility](#Fix-random-seed-for-reproducibility)\n","6. [Defining PyTorch model architecture](#Defining-PyTorch-model-architecture)\n","7. [Dataset path and hyperparameters](#Dataset-path-and-hyperparameters)\n","8. [PyTorch Dataloader](#PyTorch-Dataloader)\n","9. [--- PyTorch pre-trained ViT model loading ---](#----PyTorch-pre-trained-ViT-model-loading----)\n","10. [PyTorch model inference metrics](#PyTorch-model-inference-metrics)\n","11. [Keras model loading](#Keras-model-loading)\n","12. [--- Keras ViT pre-trained model loading ---](#----Keras-pre-trained-ViT-model-loading----)\n","13. [Define dataloader](#Define-dataloader)\n","14. [Collecting metrics for Keras-based CNN-ViT hybrid model](#Collecting-metrics-for-Keras-based-CNN-ViT-hybrid-model)\n","15. [Import the evaluation metrics](#Import-the-evaluation-metrics)\n","16. [Keras metrics reporting](#Keras-metrics-reporting)\n","17. [PyTorch metrics reporting](#PyTorch-metrics-reporting)\n","18. [ROC curve plotting](#ROC-curve-plotting)\n","19. [Comparing model performance](#Comparing-model-performance)\n","20. [Summary and discussion](#Summary-and-discussion)\n","21. [Conclusion](#Conclusion)\n"]},{"cell_type":"markdown","id":"43ac609a-f339-43b8-8d43-9b8b1c644d6e","metadata":{},"outputs":[],"source":["## Dataset download, extraction and paths\n","We begin by downloading the dataset for evaluation of the models.\n","Here, you declare:\n","1. The dataset URL from where the dataset would be downloaded.\n","2. The dataset downloading primary function, based on `skillsnetwork` library.\n","3. The dataset fallback downloading function, based on regular `http` downloading functions.\n"]},{"cell_type":"markdown","id":"2712a5c2-0b86-4386-ba6c-0fc6c35aa48b","metadata":{},"outputs":[],"source":["### Define root directory and download `url`\n","\n","First, you define the root directory where all the data would be downloaded and extracted.\n","Here, the `dataset_url` is assigned a direct link to a tar archive hosted on IBM's cloud storage. This URL points to the satellite image dataset used for land classification tasks. Using a cloud-based URL ensures accessibility without local storage dependencies. This setup facilitates automated downloads later in the notebook. \n","\n","If dealing with large datasets, monitor download times and implement retry mechanisms for robustness. This variable is key for the subsequent download functions, linking external data to the local workflow.\n"]},{"cell_type":"code","id":"4328dd12-5fb7-4f5a-a35a-006b02bc7b55","metadata":{},"outputs":[],"source":["data_dir = \".\""]},{"cell_type":"code","id":"ab4c4e32-bfa6-4d76-be83-66fb7bab803f","metadata":{},"outputs":[],"source":["dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\""]},{"cell_type":"markdown","id":"6bf78f83-c2a4-48cb-90d7-11904e631f32","metadata":{},"outputs":[],"source":["### Data download\n","We begin by downloading the dataset for evaluation of the models.\n","Here, you declare:\n","1. The dataset downloading primary function, based on `skillsnetwork` library.\n","2. The dataset fallback downloading function, based on regular `http` downloading functions.\n","3. Download the dataset\n"]},{"cell_type":"code","id":"d1a0b516-bd0a-4f1d-a677-79cb8c142a44","metadata":{},"outputs":[],"source":["import os\nimport skillsnetwork\n\ndef check_skillnetwork_extraction(extract_dir):\n    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n    if not os.path.exists(symlink_test):\n        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n        os.unlink(symlink_test)\n\nasync def download_tar_dataset(url, tar_path, extract_dir):\n    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n    if not os.path.exists(tar_path):\n        try:\n            print(f\"Downloading from {url}...\")\n            import httpx\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url, follow_redirects=True)\n                response.raise_for_status()\n                with open(tar_path, \"wb\") as f:\n                    f.write(response.content)\n            print(f\"Successfully downloaded '{tar_path}'.\")\n        except Exception as e:\n            print(f\"Download error: {e}\")\n    else:\n        print(f\"Dataset tar file already exists at: {tar_path}\")\n    import tarfile\n    with tarfile.open(tar_path, 'r:*') as tar_ref:\n        tar_ref.extractall(path=extract_dir)\n        print(f\"Successfully extracted to '{extract_dir}'.\")\n\ntry:\n    check_skillnetwork_extraction(data_dir)\n    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\nexcept Exception as e:\n    print(e)\n    print(\"Primary download/extraction method failed.\")\n    print(\"Falling back to manual download and extraction...\")\n    import tarfile\n    import httpx\n    from pathlib import Path\n    file_name = Path(dataset_url).name\n    tar_path = os.path.join(data_dir, file_name)\n    await download_tar_dataset(dataset_url, tar_path, data_dir)"]},{"cell_type":"markdown","id":"3e38797e-1cc6-4566-b912-4ab3fc4129bb","metadata":{},"outputs":[],"source":["## Pre-trained model download \n","\n","Now, define an asynchronous function to download model files from given URLs, if they are not already present locally. \n","You use `httpx` for asynchronous HTTP requests with error handling.\n"]},{"cell_type":"code","id":"eb759ece-0fc4-41f2-8893-67ee8e1b375e","metadata":{},"outputs":[],"source":["async def download_model(url, model_path):\n    if not os.path.exists(model_path):\n        try:\n            print(f\"Downloading from {url}...\")\n            import httpx\n            async with httpx.AsyncClient() as client:\n                response = await client.get(url, follow_redirects=True)\n                response.raise_for_status()\n                with open(model_path, \"wb\") as f:\n                    f.write(response.content)\n            print(f\"Successfully downloaded '{model_path}'.\")\n        except Exception as e:\n            print(f\"Download error: {e}\")\n    else:\n        print(f\"Model file already downloaded at: {model_path}\")"]},{"cell_type":"markdown","id":"3b494e38-122a-47cb-9ffc-f441defebcb9","metadata":{},"outputs":[],"source":["## Model paths and download\n","\n","In the cell below, you define the file paths and URLs for the Keras and PyTorch models and download them using the `download_model` function defined above.\n"]},{"cell_type":"code","id":"9648e2db-9c22-4ad8-9cd2-96c8a3a90af3","metadata":{},"outputs":[],"source":["data_dir = \".\"\n\nkeras_model_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7uNMQhNyTA8qSSDGn5Cc7A/keras-cnn-vit-ai-capstone.keras\"\nkeras_model_name = \"keras_cnn_vit_ai_capstone.keras\"\nkeras_model_path = os.path.join(data_dir, keras_model_name)\n\npytorch_state_dict_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/rFBrDlu1NNcAzir5Uww8eg/pytorch-cnn-vit-ai-capstone-model-state-dict.pth\"\npytorch_state_dict_name = \"pytorch_cnn_vit_ai_capstone_model_state_dict.pth\"\npytorch_state_dict_path = os.path.join(data_dir, pytorch_state_dict_name)\n\nawait download_model(keras_model_url, keras_model_path)\nawait download_model(pytorch_state_dict_url, pytorch_state_dict_path)"]},{"cell_type":"markdown","id":"554fc1ba-7605-41dd-8416-9872fe7365ff","metadata":{},"outputs":[],"source":["## Package installation\n","\n","Install the required basic Python packages. \n"]},{"cell_type":"code","id":"6fce42d8-eb51-4370-b39b-70cde6d8d7f7","metadata":{},"outputs":[],"source":["%%time\n%%capture captured_output\n%pip install numpy==1.26\n%pip install matplotlib==3.9.2\n%pip install skillsnetwork"]},{"cell_type":"markdown","id":"e6f20a2c-764c-4c2f-807a-8c0c2e1df7fe","metadata":{},"outputs":[],"source":["### Install PyTorch library\n"]},{"cell_type":"code","id":"ed97800d-5dc4-475f-afa7-9ef340bfa4e3","metadata":{},"outputs":[],"source":["%%time\n%pip install torch==2.7.0"]},{"cell_type":"markdown","id":"a9189452-23ea-4264-a8c5-9d77422b24d6","metadata":{},"outputs":[],"source":["### Install PyTorch helper libraries\n"]},{"cell_type":"code","id":"8d94c012-4878-47d5-b95a-257b9c8955ab","metadata":{},"outputs":[],"source":["%%time\n%pip install torchvision==0.22"]},{"cell_type":"markdown","id":"ee1d1074-46de-4f29-b391-cc627e1b49e0","metadata":{},"outputs":[],"source":["### Install tensorflow library for Keras\n"]},{"cell_type":"code","id":"bfb4a169-226b-4a41-ac54-be5ab3ceabb4","metadata":{},"outputs":[],"source":["%%time\n%pip install tensorflow==2.19"]},{"cell_type":"markdown","id":"5647729b-4aea-4f56-a6c1-70d6ff62d2dd","metadata":{},"outputs":[],"source":["### Install SkLearn library for evaluation metrics\n"]},{"cell_type":"code","id":"01c21d29-bcea-446d-8236-fedc1f3cc057","metadata":{},"outputs":[],"source":["%%time\n%pip install scikit-learn==1.7.0"]},{"cell_type":"markdown","id":"42663db0-bdc9-405e-84e0-2dcb0b9afe2e","metadata":{},"outputs":[],"source":["## Library imports and setup\n","\n","Import essential libraries for data manipulation, visualization, and suppresses warnings for cleaner notebook output.\n"]},{"cell_type":"code","id":"9ac28634-6fb5-4c66-8c49-134410161259","metadata":{},"outputs":[],"source":["%%time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport time\nimport httpx\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n"]},{"cell_type":"markdown","id":"64767c7d-a0de-41bc-8fce-f63ac6d850e5","metadata":{},"outputs":[],"source":["### TensorFlow/Keras library imports\n","\n","These imports set the environment variables to reduce TensorFlow logging noise and imports Keras modules for model building and training. They detect GPU availability for device assignment.\n"]},{"cell_type":"code","id":"f1a7088a-aef8-45a7-a211-aebf8c92845d","metadata":{},"outputs":[],"source":["%%time\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import load_model\n\ngpu_list = tf.config.list_physical_devices('GPU')\ndevice = \"gpu\" if gpu_list != [] else \"cpu\"\nprint(f\"TensorFlow {tf.__version__}  |  GPUs found: {tf.config.list_physical_devices('GPU')}\")"]},{"cell_type":"markdown","id":"e0a7f7fe-05a3-4bc6-be79-204768d53c70","metadata":{},"outputs":[],"source":["### PyTorch library imports\n","\n","Import core PyTorch modules for model building, optimization, data loading, and functional utilities.\n"]},{"cell_type":"code","id":"4598f817-9ee7-41ed-80f6-f72d5d878e70","metadata":{},"outputs":[],"source":["%%time\nimport torch\nimport torch.nn as nn\n#import torch.optim as optim\nfrom torchvision import transforms\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import  random_split\nimport torch.nn.functional as F\n\nprint(\"Imported libraries\")"]},{"cell_type":"markdown","id":"eda7901c-a19f-49c3-b18b-01d9111c7519","metadata":{},"outputs":[],"source":["## Fix random seed for reproducibility\n","\n","Define `set_seed` to ensure reproducibility across Python, NumPy, TensorFlow, and PyTorch by seeding random generators and enabling deterministic cuDNN. \n","\n","Set `SEED` to 7331. This is useful for consistent results in stochastic processes like training or inference.\n"]},{"cell_type":"code","id":"f7fc77bc-62b0-4fa0-840c-5ac706b7f656","metadata":{},"outputs":[],"source":["#====================\ndef set_seed(seed: int = 42) -> None:\n    \"\"\"Seed Python, NumPy, tensorflow, and PyTorch (CPU & all GPUs) and\n    make cuDNN run in deterministic mode.\"\"\"\n    # ---- Python and NumPy -------------------------------------------\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # ---- Tensorflow -------------------------------------------------\n    tf.random.set_seed(seed)\n\n    # ---- PyTorch (CPU  &  GPU) --------------------------------------\n    torch.manual_seed(seed)            \n    torch.cuda.manual_seed_all(seed)   \n\n    # ---- cuDNN: force repeatable convolutions -----------------------\n    torch.backends.cudnn.deterministic = True \n    torch.backends.cudnn.benchmark     = False \n\n#====================\nSEED = 7331\nset_seed(SEED)\nprint(f\"Global seed set to {SEED} - Processes are now deterministic.\")"]},{"cell_type":"markdown","id":"192e5a89-29a7-46a1-8361-b93cd20fbd82","metadata":{},"outputs":[],"source":["## Model paths \n","Check for the existence of the Keras and PyTorch model files. This ensures models are accessible before loading, preventing runtime errors. Use absolute paths for reliability.\n"]},{"cell_type":"code","id":"2559240b-7d80-4669-9729-096340640d9a","metadata":{},"outputs":[],"source":["if not os.path.exists(keras_model_path):\n    print(\"Unable to find the Keras model at give path. Please check...\")\nelse:\n    print(f\"Found the pre-trained Keras model:\\n{keras_model_name} --at------> {keras_model_path}\")\n\nif not os.path.exists(pytorch_state_dict_path):\n    print(\"Unable to find the PyTorch model at give path. Please check...\")\nelse:\n    print(f\"Found the pre-trained PyTorch model:\\n{pytorch_state_dict_name} --at------> {pytorch_state_dict_path}\")"]},{"cell_type":"markdown","id":"4ea2fd2c-a24b-4040-9542-41b6c938e284","metadata":{},"outputs":[],"source":["## Defining PyTorch model architecture\n","In this cell, you will define the PyTorch CNN-ViT model architegcture, exactly as defined during the model training. You define the classes for CNN feature extractor, patch embedding, multi-head self-attention, transformer block, ViT, and CNN-ViT hybrid. \n","\n","The `evaluate` function computes loss and accuracy. This architecture combines CNN local features with ViT global attention. \n","\n","Parameters like depth and heads are configurable, and defined same as during training.\n"]},{"cell_type":"code","id":"1323f0d4-7166-4a69-a00d-1d4917741269","metadata":{},"outputs":[],"source":["#====================\nclass ConvNet(nn.Module):\n    ''' \n    Class to define the architecture same as the imported pre-trained CNN model\n    '''\n    def __init__(self, num_classes: int):\n        super().__init__()\n        # -------- convolutional feature extractor --------\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32,  kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(32),\n            nn.Conv2d(32, 64,  kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n            nn.Conv2d(64, 128, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n            nn.Conv2d(128, 256, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n            nn.Conv2d(256, 512, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n            nn.Conv2d(512, 1024, kernel_size=5, padding=2), nn.ReLU(inplace=True), nn.MaxPool2d(2), nn.BatchNorm2d(1024),\n        )\n\n        # -------- global pooling + classifier head --------\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = nn.Sequential(nn.Flatten(),                           # flatten feature map of dimensions (1024 × 1 × 1) to 1024\n                                        nn.Linear(1024, 2048), nn.ReLU(inplace=True), nn.BatchNorm1d(2048), nn.Dropout(0.4), \n                                        nn.Linear(2048, num_classes)\n                                       )\n\n    def forward_features(self, x: torch.Tensor) -> torch.Tensor:\n        return self.features(x)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.forward_features(x)   # features, dimensions:(B, 1024, H', W')\n        x = self.pool(x)               # global-average-pooling, dimensions: (B, 1024, 1, 1)\n        x = self.classifier(x)         # classifier, dimensions: (B, num_classes)\n        return x\n\n#====================\nclass PatchEmbed(nn.Module):\n    def __init__(self, input_channel=1024, embed_dim=768):\n        super().__init__()\n        self.proj = nn.Conv2d(input_channel, embed_dim, kernel_size=1)  # 1×1 conv\n    \n    def forward(self, x):\n        x = self.proj(x).flatten(2).transpose(1, 2)  # (B,L,D)\n        return x\n\n#====================\nclass MHSA(nn.Module):\n    def __init__(self, dim, heads=8, dropout=0.):\n        super().__init__()\n        self.heads = heads\n        self.scale = (dim // heads) ** -0.5\n        self.qkv = nn.Linear(dim, dim * 3)\n        self.attn_drop = nn.Dropout(dropout)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        B, N, D = x.shape\n        q, k, v = self.qkv(x).chunk(3, dim=-1)\n        q = q.reshape(B, N, self.heads, -1).transpose(1, 2)  # (B, heads, N, d)\n        k = k.reshape(B, N, self.heads, -1).transpose(1, 2)\n        v = v.reshape(B, N, self.heads, -1).transpose(1, 2)\n        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n        attn = self.attn_drop(attn.softmax(dim=-1))\n        x = torch.matmul(attn, v).transpose(1, 2).reshape(B, N, D)\n        return self.proj_drop(self.proj(x))\n\n#====================\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, heads, mlp_ratio=4., dropout=0.):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn  = MHSA(dim, heads, dropout)\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp   = nn.Sequential(\n                                    nn.Linear(dim, int(dim * mlp_ratio)),\n                                    nn.GELU(), nn.Dropout(dropout),\n                                    nn.Linear(int(dim * mlp_ratio), dim),\n                                    nn.Dropout(dropout))\n    \n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n#====================\nclass ViT(nn.Module):\n    def __init__(self, in_ch=1024, num_classes=2,\n                 embed_dim=768, depth=6, heads=8,\n                 mlp_ratio=4., dropout=0.1, max_tokens=50):\n        super().__init__()\n        self.patch = PatchEmbed(in_ch, embed_dim)           # 1×1 conv\n        self.cls   = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos   = nn.Parameter(torch.randn(1, max_tokens, embed_dim))\n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, heads, mlp_ratio, dropout)\n            for _ in range(depth)])\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):                          # x: (B,C,H,W)\n        x = self.patch(x)                          # (B,L,D)\n        B, L, _ = x.shape\n        cls = self.cls.expand(B, -1, -1)           # (B,1,D)\n        x = torch.cat((cls, x), 1)                 # (B,L+1,D)\n        x = x + self.pos[:, :L + 1]                # match seq-len\n        for blk in self.blocks:\n            x = blk(x)\n        return self.head(self.norm(x)[:, 0])       # CLS token\n\n#====================\nclass CNN_ViT_Hybrid(nn.Module):\n    def __init__(self, num_classes=2, embed_dim=768, depth=6, heads=8):\n        super().__init__()\n        self.cnn = ConvNet(num_classes)            # load weights later\n        self.vit = ViT(num_classes=num_classes,\n                       embed_dim=embed_dim,\n                       depth=depth,\n                       heads=heads)\n    \n    def forward(self, x):\n        return self.vit(self.cnn.forward_features(x))\n\n#====================\ndef evaluate(model, loader, criterion, device):\n    with torch.no_grad():\n        model.eval()\n        loss_sum, correct = 0, 0\n        for batch_idx, (x, y) in enumerate(tqdm(loader, desc=\"Validation\")):\n            x, y = x.to(device), y.to(device)\n            out = model(x)\n            loss = criterion(out, y)\n            loss_sum += loss.item() * x.size(0)\n            correct  += (out.argmax(1) == y).sum().item()\n    return loss_sum / len(loader.dataset), correct / len(loader.dataset)# Set device"]},{"cell_type":"markdown","id":"6ebf2645-d6a2-4d13-b16a-42bc862ebd25","metadata":{},"outputs":[],"source":["## Dataset path and hyperparameters\n","Here, you set the dataset path and hyperparameters like image size, channels, batch size, classes, and labels. These are used for data loading and model configuration. Consistent dimensions ensure compatibility with model inputs.\n"]},{"cell_type":"markdown","id":"b6e2dfca-3375-492f-b63c-286614dc4141","metadata":{},"outputs":[],"source":["## Task 1: Define the dataset directory, dataloader and model hyperparameters. The dataloader and model hyperparameters should be same as used during training \n","\n","- Define the `dataset_path`\n","\n","- Define **hyperparameters common dataloader**\n","    - `img_w`, `img_h = 64, 64`\n","    - `batch_size = 128`\n","    - `num_classes = 2`\n","    - `agri_class_labels = [\"non-agri\", \"agri\"]`\n","\n","  \n","- Define **hyperparameters for PyTorch CNN-Vit Hybrid model**. The values have to same as those used while training the hybrid model. \n","    - `depth = 3`\n","    - `attn_heads = 6`\n","    - `embed_dim = 768`\n","\n"]},{"cell_type":"code","id":"94a6c3bf-ba61-451f-bff4-de4d6d3be1c6","metadata":{},"outputs":[],"source":["## Please use the space below to write your answer\n"]},{"cell_type":"markdown","id":"2dfea0fa-aad4-4131-ab22-f18bc08e0fc6","metadata":{},"outputs":[],"source":["Double-click **here** for the solution.\n","<!--\n","dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n","\n","# hyperparameters common dataloader\n","img_w, img_h = 64, 64\n","batch_size = 128\n","num_classes = 2\n","agri_class_labels = [\"non-agri\", \"agri\"]\n","\n","# hyperparameters for PyTorch CNN-Vit Hybrid model\n","depth = 3\n","attn_heads = 6\n","embed_dim = 768\n","-->\n"]},{"cell_type":"markdown","id":"412d3916-16ba-4280-a801-5aeb507139aa","metadata":{},"outputs":[],"source":["### PyTorch Dataloader\n","Defines transforms for resizing, tensor conversion, and normalization (ImageNet means/std). \n","\n","Loads dataset with ImageFolder and creates DataLoader for batching without shuffling for evaluation.\n"]},{"cell_type":"code","id":"910598d2-f6ee-414c-b287-954a737cfac6","metadata":{},"outputs":[],"source":["train_transform = transforms.Compose([\n    transforms.Resize((img_w, img_h)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\nfull_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)\ntest_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","id":"c6fd882a-5bf1-4648-85ca-f2f48faee401","metadata":{},"outputs":[],"source":["## Task 2: Instantiate PyTorch model\n","Check the availability of CUDA device and set the `device` parameter accordingly.\n","\n","Based on the `CNN_ViT_Hybrid` function, instantiate the PyTorch model and move the model to the available `device`\n","\n","In this cell, you will:\n","1. instantiate the PyTorch CNN_ViT_Hybrid with the previously declared model parameters\n","3. detect the device for model inference\n"," \n"]},{"cell_type":"code","id":"fbf852f7-e294-4659-a3bf-a9253f55b3cf","metadata":{},"outputs":[],"source":["## Please use the space below to write your answer\n"]},{"cell_type":"markdown","id":"f69ae950-400b-4e96-9540-b392c981cee2","metadata":{},"outputs":[],"source":["Double-click **here** for the solution.\n","<!--\n","# Check device availability\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Create model instance\n","pytorch_model = CNN_ViT_Hybrid(num_classes=num_classes,\n","                      heads=attn_heads,\n","                      depth=depth,\n","                      embed_dim=embed_dim).to(device)\n","-->\n"]},{"cell_type":"code","id":"0d88c7b0-0b23-4e30-88f3-89a4eadf0e75","metadata":{},"outputs":[],"source":["print(f\"Evaluating the PyTorch model on {device}\")"]},{"cell_type":"markdown","id":"8e6c1686-5418-42bb-89ac-875235171d78","metadata":{},"outputs":[],"source":["### --- PyTorch pre-trained ViT model loading ---\n","In this cell, you will load the PyTorch model state dict with **`strict=False`** for flexibility.\n","\n","Thus, you prepare the model for inference.\n"]},{"cell_type":"code","id":"53f4f714-efe9-43a1-b58b-82e7c908e32f","metadata":{},"outputs":[],"source":["# Load pre-trained CNN-ViT hybrid model weights \nif device==\"cpu\":\n    map_location=torch.device(\"cpu\")\nelse:\n    map_location=torch.device(\"cuda\")\n\npytorch_model.load_state_dict(torch.load(pytorch_state_dict_path, map_location=map_location), strict=False)\nprint(\"Loaded model state dict, now getting predictions\")"]},{"cell_type":"markdown","id":"9aaf05aa-6ec2-495a-aa05-7a03b83fd73f","metadata":{},"outputs":[],"source":["### PyTorch model inference metrics\n","\n","Now, you perform:\n","1. inference on test_loader\n","2. collecte prediction, labels, and probabilities (for class 1)\n","3. Uses no_grad for efficiency and eval mode\n","4. Use tqdm to show progress.\n","5. Move the data to the training device (CPU/GPU).\n"]},{"cell_type":"code","id":"6c153499-357c-41ca-bc5b-3b965be5f80f","metadata":{},"outputs":[],"source":["%%time\nall_preds_pytorch = []\nall_labels_pytorch = []\nall_probs_pytorch = []\n\npytorch_model.eval()\nwith torch.no_grad():\n    for batch_idx, (images, labels) in enumerate(tqdm(test_loader, desc=\"Step\")):\n#    for images, labels in test_loader:\n        images = images.to(device)\n        outputs = pytorch_model(images)\n        preds = torch.argmax(outputs, dim=1)\n        probs = F.softmax(outputs, dim=1)[:, 1]  # probability for class 1\n        all_probs_pytorch.extend(probs.cpu())\n        all_preds_pytorch.extend(preds.cpu().numpy().flatten())\n        all_labels_pytorch.extend(labels.numpy())"]},{"cell_type":"markdown","id":"666b8422-b12b-480d-9cef-34ad6bc6f593","metadata":{},"outputs":[],"source":["## Keras model loading\n","\n","To load the Keras based CNN-ViT hybrid model, you will\n","\n","- define **custom Keras layers** with serialization for model saving/loading for:\n","    - `position embedding`\n","    - `transformer block`\n","\n","This step is essential for reconstructing the ViT architecture in Keras.\n"]},{"cell_type":"code","id":"ade47e11-cc55-400e-aa26-6eafda2df37e","metadata":{},"outputs":[],"source":["# Positional embedding that Keras can track\n@tf.keras.utils.register_keras_serializable(package=\"Custom\")\nclass AddPositionEmbedding(layers.Layer):\n    def __init__(self, num_patches, embed_dim, **kwargs):\n        super(AddPositionEmbedding, self).__init__(**kwargs)\n        self.num_patches = num_patches\n        self.embed_dim   = embed_dim\n        self.pos = self.add_weight(\n            name=\"pos_embedding\",\n            shape=(1, num_patches, embed_dim),\n            initializer=\"random_normal\",\n            trainable=True)\n\n    def call(self, tokens):\n        return tokens + self.pos\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"num_patches\": self.num_patches,\n            \"embed_dim\":   self.embed_dim,\n        })\n        return {**config}\n\n# One Transformer encoder block\n@tf.keras.utils.register_keras_serializable(package=\"Custom\")\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads=8, mlp_dim=2048, dropout=0.1, **kwargs):\n        super(TransformerBlock, self).__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.mlp_dim   = mlp_dim\n        self.dropout   = dropout\n        self.mha  = layers.MultiHeadAttention(num_heads, key_dim=embed_dim)\n        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.mlp = tf.keras.Sequential([\n            layers.Dense(mlp_dim, activation=\"gelu\"),\n            layers.Dropout(dropout),\n            layers.Dense(embed_dim),\n            layers.Dropout(dropout)\n        ])\n\n    def call(self, x):\n        x = self.norm1(x + self.mha(x, x))\n        return self.norm2(x + self.mlp(x))\n\n    # ---- NEW ----\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\":  self.embed_dim,\n            \"num_heads\":  self.num_heads,\n            \"mlp_dim\":    self.mlp_dim,\n            \"dropout\":    self.dropout,\n        })\n        return {**config}"]},{"cell_type":"markdown","id":"a100e263-c505-43f6-874c-f8119d6405ca","metadata":{},"outputs":[],"source":["### --- Keras pre-trained ViT model loading ---\n","\n","Here, you will load the pre-trained Keras model using **`load_model`**, providing **custom objects** for deserialization of user-defined layers. This enables inference with the hybrid model.\n"]},{"cell_type":"code","id":"b8581db4-0af0-4ef5-9e7c-6eb9755638e1","metadata":{},"outputs":[],"source":["# ------------------- load CNN-ViT hybrid model ------------------\nkeras_model = load_model(keras_model_name,\n                         custom_objects={\n                         \"AddPositionEmbedding\": AddPositionEmbedding,\n                         \"TransformerBlock\":     TransformerBlock\n                          })"]},{"cell_type":"markdown","id":"c3dbc00b-dd9a-4ab8-86ef-c31ff269e3af","metadata":{},"outputs":[],"source":["### Define dataloader\n","\n","In this cell, you create an ImageDataGenerator for rescaling and a generator for flowing images from directory, matching PyTorch setup for consistent evaluation.\n"]},{"cell_type":"code","id":"8992cc89-7afa-4f05-afdf-8ff69dfd1f14","metadata":{},"outputs":[],"source":["datagen = ImageDataGenerator(rescale=1./255)\nprediction_generator = datagen.flow_from_directory(\n    dataset_path,\n    target_size=(img_w, img_h),\n    batch_size=batch_size,\n    class_mode=\"binary\",\n    shuffle=False\n)"]},{"cell_type":"markdown","id":"c975ce09-2fc1-461e-afcb-c2f8a922382a","metadata":{},"outputs":[],"source":["### Collecting metrics for Keras-based CNN-ViT hybrid model\n","Now, run the inference of the Keras-based CNN-ViT hybrid model and collect the evaluation metrics.\n"]},{"cell_type":"code","id":"07247681-d31d-4634-8e6f-d395fd92c491","metadata":{},"outputs":[],"source":["%%time\n\nall_probs_keras = keras_model.predict(prediction_generator, verbose=1)\nall_preds_keras = np.argmax(all_probs_keras, axis=1)\nall_labels_keras = prediction_generator.classes"]},{"cell_type":"markdown","id":"390e8a5c-2268-49dc-86a3-2eff1d36c256","metadata":{},"outputs":[],"source":["## Import the evaluation metrics\n","\n","Here you define the functions to compute and print classification metrics including accuracy, precision, recall, F1 score, ROC-AUC, confusion matrix, and log loss. These functions support both Keras and PyTorch model outputs.\n"]},{"cell_type":"code","id":"6f0a1d05-a2b4-4dcc-a248-af4ea4ba201f","metadata":{},"outputs":[],"source":["%%time\nfrom sklearn.metrics import (accuracy_score,\n                             precision_score,\n                             recall_score,\n                             f1_score,\n                             roc_curve, \n                             roc_auc_score,\n                             log_loss,\n                             classification_report,\n                             confusion_matrix,\n                             ConfusionMatrixDisplay,\n                            )\nfrom sklearn.preprocessing import label_binarize\n\n# define a function to get the metrics comprehensively\ndef model_metrics(y_true, y_pred, y_prob, class_labels):\n    y_prob = np.array(y_prob)\n    if len(y_prob.shape)<2:\n        roc_score = roc_auc_score(y_true, y_prob)\n    elif len(y_prob.shape)==2:\n        roc_score = roc_auc_score(y_true, y_prob[:,1])\n    else:\n        roc_score = np.nan\n    metrics = {'Accuracy': accuracy_score(y_true, y_pred),\n               'Precision': precision_score(y_true, y_pred),\n               'Recall': recall_score(y_true, y_pred),\n               'Loss': log_loss(y_true, y_prob),\n               'F1 Score': f1_score(y_true, y_pred),\n               'ROC-AUC': roc_score,\n               'Confusion Matrix': confusion_matrix(y_true, y_pred),\n               'Classification Report': classification_report(y_true, y_pred, target_names=class_labels, digits=4),\n               \"Class labels\": class_labels\n              }\n    return metrics\n\n#function to print the metrics\ndef print_metrics(y_true, y_pred, y_prob, class_labels, model_name):\n    metrics = model_metrics(y_true, y_pred, y_prob, class_labels)\n    \n    print(f\"Evaluation metrics for the \\033[1m{model_name}\\033[0m\")\n    print(f\"Accuracy: {'':<1}{metrics[\"Accuracy\"]:.4f}\")\n    if metrics[\"ROC-AUC\"] != np.nan:\n        print(f\"ROC-AUC: {'':<2}{metrics[\"ROC-AUC\"]:.4f}\")\n    print(f\"Loss: {'':<5}{metrics[\"Loss\"]:.4f}\\n\")\n    print(f\"Classification report:\\n\\n  {metrics[\"Classification Report\"]}\")\n    print(\"========= Confusion Matrix =========\")\n    disp = ConfusionMatrixDisplay(confusion_matrix=metrics[\"Confusion Matrix\"],\n                                  display_labels=metrics[\"Class labels\"])\n\n    disp.plot()\n    plt.show()\n"]},{"cell_type":"markdown","id":"15cef919-6ed3-439a-a0d1-4d34b72d4df7","metadata":{},"outputs":[],"source":["## Keras metrics reporting\n"]},{"cell_type":"markdown","id":"d07fbfa7-2637-4c40-843b-4dd6789be9a6","metadata":{},"outputs":[],"source":["## Task 3: Print the evaluation metrics using `print_metrics` function for the **Keras** ViT model with name `Keras CNN-Vit Hybrid Model`\n"]},{"cell_type":"code","id":"679cc435-66c5-4788-bc82-8eeed4fa2a35","metadata":{},"outputs":[],"source":["## Please use the space below to write your answer\n"]},{"cell_type":"markdown","id":"456f2e5b-3e13-45a1-aa61-79792eabbe70","metadata":{},"outputs":[],"source":["Double-click **here** for the solution.\n","<!--\n","print_metrics(y_true = all_labels_keras,\n","              y_pred = all_preds_keras,\n","              y_prob = all_probs_keras,\n","              class_labels = agri_class_labels,\n","              model_name = \"Keras CNN-Vit Hybrid Model\"\n","             )\n","-->\n"]},{"cell_type":"markdown","id":"6a720a07-24af-4c9d-bed6-d794b7f67e43","metadata":{},"outputs":[],"source":["## PyTorch metrics reporting\n"]},{"cell_type":"markdown","id":"a7545ccf-1727-4105-993f-8081f063aef3","metadata":{},"outputs":[],"source":["## Task 4: Print the evaluation metrics using `print_metrics` function for the **PyTorch** ViT model with model name `PyTorch CNN-Vit Hybrid Model`\n"]},{"cell_type":"code","id":"815401b7-9ea8-42a5-9045-5fd55ef5993a","metadata":{},"outputs":[],"source":["## Please use the space below to write your answer\n"]},{"cell_type":"markdown","id":"73412960-ab7e-42ee-bf75-d72a85d931b6","metadata":{},"outputs":[],"source":["Double-click **here** for the solution.\n","<!--\n","print_metrics(y_true = all_labels_pytorch,\n","              y_pred = all_preds_pytorch,\n","              y_prob = np.array(all_probs_pytorch),\n","              class_labels = agri_class_labels,\n","              model_name = \"PyTorch CNN-Vit Hybrid Model\"\n","             )\n","-->\n"]},{"cell_type":"markdown","id":"c15339c4-e0a9-4d78-9c36-8638492f8b02","metadata":{},"outputs":[],"source":["## ROC curve plotting\n","\n","First, define a function to plot ROC curves for binary or multi-class classification using scikit-learn's `roc_curve` and `roc_auc_score`. It handles both single-class and multi-class cases by binarizing labels if needed.\n","\n","Next, plot the ROC curves for both the models.\n"]},{"cell_type":"code","id":"eb4b0fab-9bb0-49ed-b4c5-86317d07be6a","metadata":{},"outputs":[],"source":["\ndef plot_roc(y_true, y_prob, model_name):\n    n_classes = y_prob.shape[1] if y_prob.ndim > 1 else 1\n    if n_classes == 1:\n        fpr, tpr, _ = roc_curve(y_true, y_prob)\n        auc = roc_auc_score(y_true, y_prob)\n        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc:.4f})')\n    else:\n        y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n        for i in range(n_classes):\n            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n            auc = roc_auc_score(y_true_bin[:, i], y_prob[:, i])\n            plt.plot(fpr, tpr, label=f'{model_name} class {i} (AUC = {auc:.4f})')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend()"]},{"cell_type":"markdown","id":"c35d5d57-f6c1-4597-a63d-0d645889cbb5","metadata":{},"outputs":[],"source":["### ROC curve plotting for both models\n","\n","Plot the ROC curves for both Keras and PyTorch models on the same figure for visual performance comparison.\n"]},{"cell_type":"code","id":"c084826b-d932-4907-b3bb-37db7ef846da","metadata":{},"outputs":[],"source":["plot_roc(np.array(all_labels_keras), np.array(all_probs_keras[:, 1]), \"Keras Model\")\nplt.show()\nplot_roc(np.array(all_labels_pytorch), np.array(all_probs_pytorch), \"PyTorch Model\")\nplt.show()"]},{"cell_type":"markdown","id":"fc6a09ca-1d8f-46cb-ac04-ba785f3baeb6","metadata":{},"outputs":[],"source":["## Comparing model performance\n","\n","Now compare the performance of different models to understand which model would be the best performer for your land classification task.\n","Computed metrics for both models are used to generate a comparison table for key scores. This facilitates quick performance assessment between frameworks.\n"]},{"cell_type":"code","id":"ab5bf6b0-8c55-4bda-8c50-62906e5a1912","metadata":{},"outputs":[],"source":["# get the Keras model performance metrics\nmetrics_keras = model_metrics(all_labels_keras, all_preds_keras, all_probs_keras, agri_class_labels)\n\n# get the PyTorch model performance metrics\nmetrics_pytorch = model_metrics(all_labels_pytorch, all_preds_pytorch, all_probs_pytorch, agri_class_labels)\n\n\n# Display the comparison of metrics\nprint(\"{:<18} | {:<15} {:<15}\".format('\\033[1m'+ 'Metric' + '\\033[0m',\n                                    'Keras Model', \n                                    'PyTorch Model'))\nprint((\"\".join([\"-\" for _ in range(43)])))\nmetrics_list = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC-AUC']\n\nfor k in metrics_list:\n    print(\"{:<18} | {:<15.4f} {:<15.4f}\".format('\\033[1m'+k+'\\033[0m',\n                                              metrics_keras[k],\n                                              metrics_pytorch[k]))"]},{"cell_type":"markdown","id":"e61cd795-df29-4069-8996-badc8c5efe47","metadata":{},"outputs":[],"source":["## Summary and discussion\n","\n","This notebook showcased a framework-agnostic workflow for importing, testing, and evaluating Vision Transformer models built in both Keras and PyTorch. By running the same input through each model, we examined the compatibility of results and gained practical experience handling architectural and data format variations.\n","\n","Key insights include the criticality of input format alignment, the subtle differences in model serialization/loading, and the framework-induced variations in prediction outputs. For a more robust evaluation, repeat this process with a labeled validation dataset, compute further metrics (precision, recall, F1-score), and systematically analyze speed and resource usage.\n","\n"]},{"cell_type":"markdown","id":"cdfe3729-0a65-4660-ab1f-9c16018fcd06","metadata":{},"outputs":[],"source":["## Save and download the notebook for **final project** submission and evaluation\n","\n","You will need to save and download the completed notebook for final project submission and evaluation. \n","<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n","\n","<font size = 4>  \n","\n","1) **Complete** all the tasks and questions given in the notebook.\n","\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n","\n","2) **Save** the notebook.</style>\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n","\n","3) Identify and right click on the **correct notebook file** in the left pane.</style>\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n","\n","4) Click on **Download**.</style>\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n","\n","5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n","<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n","  </font>\n"]},{"cell_type":"markdown","id":"77331c94-a2d3-4eed-89a9-3ad2af60e5dc","metadata":{},"outputs":[],"source":["## Conclusion\n","Congratulations! You have successfully completed this lab and now you have a good understanding about loading custom pre-trained models, for both Keras and PyTorch frameworks. Using these pre-trained models, you can now evaluate their performance and also infer unknown datasets.\n","I hope you have learnt to apply the key concepts of Keras/Pytorch based classifiers, both traditional CNNs and more advanced and state-of-the-art, CNN-ViT hybrid models. Using this knowledge, now you should be able to tackle a variety of real world image classification problems. Good luck!\n"]},{"cell_type":"markdown","id":"50d8537b-0af3-4d0e-979a-4c81923dd910","metadata":{},"outputs":[],"source":["<h2>Author</h2>\n","\n","[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n","\n","Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n"]},{"cell_type":"markdown","id":"5caed908-4c54-4843-a13a-6a60b07a1679","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2025-07-28  | 1.0  | Aman  |  Created the lab |\n","\n","-->\n"]},{"cell_type":"markdown","id":"4bcbb89a-06df-4c5a-9eb5-2478a7922db1","metadata":{},"outputs":[],"source":["© Copyright IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"prev_pub_hash":"1f89431c78481922f903e466e3093716dda75a39461d909b555f4854b9af0f71"},"nbformat":4,"nbformat_minor":4}